{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6256b93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best CV Score: 0.8339\n",
      "Best Parameters: {'model__max_depth': 5, 'model__max_features': 'log2', 'model__min_samples_leaf': 4, 'model__min_samples_split': 15, 'model__n_estimators': 585}\n",
      "Saved 'submission_optimized.csv'. Ready for Kaggle!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. DATA LOADING ---\n",
    "def load_titanic_data():\n",
    "    train_path = Path(\"datasets/titanic/train.csv\")\n",
    "    test_path = Path(\"datasets/titanic/test.csv\")\n",
    "    return pd.read_csv(train_path), pd.read_csv(test_path)\n",
    "\n",
    "train_data, test_data = load_titanic_data()\n",
    "y_train = train_data[\"Survived\"].copy()\n",
    "X_train = train_data.drop(\"Survived\", axis=1) # Ensure we don't cheat\n",
    "\n",
    "# --- 2. CUSTOM TRANSFORMERS ---\n",
    "\n",
    "# A. Title Selector: Extracts Mr, Mrs, Master, etc.\n",
    "class TitleSelector(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Extract title from Name\n",
    "        X['Title'] = X['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "        # Group rare titles\n",
    "        X['Title'] = X['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', \n",
    "                                         'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "        X['Title'] = X['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
    "        X['Title'] = X['Title'].replace('Mme', 'Mrs')\n",
    "        return X[['Title']]\n",
    "\n",
    "# B. Family Size: Combines SibSp + Parch + 1 (Self)\n",
    "class FamilySize(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Create FamilySize\n",
    "        family_size = X[\"SibSp\"] + X[\"Parch\"] + 1\n",
    "        return family_size.to_frame()\n",
    "\n",
    "# C. Smart Age Imputer: Fills missing Age based on Title (not global median)\n",
    "class SmartAgeImputer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate median age for each Title in the training set\n",
    "        # We need to extract titles temporarily to learn the medians\n",
    "        temp_df = X.copy()\n",
    "        temp_df['Title'] = temp_df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "        temp_df['Title'] = temp_df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', \n",
    "                                         'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "        temp_df['Title'] = temp_df['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
    "        temp_df['Title'] = temp_df['Title'].replace('Mme', 'Mrs')\n",
    "        \n",
    "        self.age_map_ = temp_df.groupby(\"Title\")[\"Age\"].median()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Extract titles again for the transform step\n",
    "        titles = X['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "        titles = titles.replace(['Lady', 'Countess','Capt', 'Col','Don', \n",
    "                                         'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "        titles = titles.replace(['Mlle', 'Ms'], 'Miss')\n",
    "        titles = titles.replace('Mme', 'Mrs')\n",
    "        \n",
    "        # Map the learnt medians to the missing values\n",
    "        def fill_age(row):\n",
    "            if pd.isna(row['Age']):\n",
    "                return self.age_map_.get(titles.iloc[row.name], X['Age'].median())\n",
    "            return row['Age']\n",
    "            \n",
    "        # We need to reset index to ensure alignment, then restore it\n",
    "        X_reset = X.reset_index(drop=True)\n",
    "        # Note: In a real pipeline, complex row-wise operations can be slow, \n",
    "        # but for Titanic (891 rows) this is fine and safe.\n",
    "        # For simplicity in this script, we'll use a simpler fillna approach:\n",
    "        \n",
    "        # Optimized Vectorized Fill:\n",
    "        for title, median_age in self.age_map_.items():\n",
    "             mask = (titles == title) & (X['Age'].isna())\n",
    "             X.loc[mask, 'Age'] = median_age\n",
    "             \n",
    "        return X[['Age']]\n",
    "\n",
    "# --- 3. PIPELINES ---\n",
    "\n",
    "# A. Numerical Pipeline (Age, Fare)\n",
    "# Note: We use SmartAgeImputer for Age, but SimpleImputer for Fare\n",
    "age_pipeline = Pipeline([\n",
    "    (\"smart_imputer\", SmartAgeImputer()),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "fare_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# B. Categorical Pipeline (Sex, Embarked)\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "# C. Title Pipeline\n",
    "title_pipeline = Pipeline([\n",
    "    (\"extractor\", TitleSelector()),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "# D. Family Pipeline\n",
    "family_pipeline = Pipeline([\n",
    "    (\"calculator\", FamilySize()),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Master Preprocessor\n",
    "preprocess_pipeline = ColumnTransformer([\n",
    "    (\"age_proc\", age_pipeline, [\"Name\", \"Age\"]), # Pass Name to get Title for Age\n",
    "    (\"fare_proc\", fare_pipeline, [\"Fare\"]),\n",
    "    (\"cat_proc\", cat_pipeline, [\"Sex\", \"Embarked\"]),\n",
    "    (\"title_proc\", title_pipeline, [\"Name\"]),\n",
    "    (\"family_proc\", family_pipeline, [\"SibSp\", \"Parch\"]),\n",
    "    (\"ord_proc\", OrdinalEncoder(), [\"Pclass\"])\n",
    "])\n",
    "\n",
    "# --- 4. MODEL & SEARCH ---\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocess_pipeline),\n",
    "    (\"model\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# We constrain the search to prevent overfitting\n",
    "param_dist = {\n",
    "    \"model__n_estimators\": randint(200, 600),\n",
    "    \"model__max_depth\": [5, 8, 10, 12],     # Capped at 12\n",
    "    \"model__min_samples_split\": [5, 10, 15], # Require more samples to split\n",
    "    \"model__min_samples_leaf\": [2, 4, 8],    # Require more samples in leaves\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    full_pipeline, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=50, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    verbose=1, \n",
    "    n_jobs=-1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best CV Score: {random_search.best_score_:.4f}\")\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# --- 5. SUBMISSION ---\n",
    "final_model = random_search.best_estimator_\n",
    "test_predictions = final_model.predict(test_data)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_data[\"PassengerId\"],\n",
    "    \"Survived\": test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_optimized.csv\", index=False)\n",
    "print(\"Saved 'submission_optimized.csv'. Ready for Kaggle!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
